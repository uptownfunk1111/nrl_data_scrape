{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NRL Feature Map and Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\slangston1\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (80.0.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.1.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
      "Requirement already satisfied: optree in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\slangston1\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\slangston1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install tensorflow\n",
    "!pip install numpy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Data from the JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = [\"Broncos\", \"Roosters\", \"Wests Tigers\", \"Rabbitohs\", \"Storm\", \"Eels\", \"Raiders\", \"Knights\", \"Dragons\", \"Sea Eagles\", \"Panthers\", \"Sharks\", \"Bulldogs\", \"Dolphins\", \"Titans\", \"Cowboys\", \"Warriors\"]\n",
    "variables =[\"Year\", \"Win\", \"Defense\", \"Attack\", \"Margin\", \"Home\", \"Versus\",  \"Round\"]\n",
    "years =  [2019, 2020, 2021, 2022, 2023, 2024, 2025]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/nrl_data_multi_years_2.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m years_arr = {}\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Open the JSON file containing NRL data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/nrl_data_multi_years_2.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Load JSON data from the file\u001b[39;00m\n\u001b[32m      7\u001b[39m     data = json.load(file)\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Extract NRL data from the loaded JSON\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../data/nrl_data_multi_years_2.json'"
     ]
    }
   ],
   "source": [
    "# Initialize an empty dictionary to store data for each year\n",
    "years_arr = {}\n",
    "\n",
    "# Open the JSON file containing NRL data\n",
    "with open('../data/nrl_data_multi_years_2.json', 'r') as file:\n",
    "    # Load JSON data from the file\n",
    "    data = json.load(file)\n",
    "    \n",
    "    # Extract NRL data from the loaded JSON\n",
    "    data = data['NRL']\n",
    "    \n",
    "    # Iterate over each year in the years list\n",
    "    for year in years:\n",
    "        # Extract data for the current year and store it in the years_arr dictionary\n",
    "        # Note: years.index(year) returns the index of the current year in the years list\n",
    "        #       This index is then used to access the corresponding data for that year\n",
    "        years_arr[year] = data[years.index(year)][str(year)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with columns representing combinations of team and variable names\n",
    "df = pd.DataFrame(columns=[f\"{team} {variable}\" for team in teams for variable in variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize an empty list to store data for all rounds\n",
    "all_store = []\n",
    "\n",
    "# Iterate over each year in the years list\n",
    "for year in years:\n",
    "    # Iterate over each round (assuming 26 rounds)\n",
    "    for round in range(0, 26):\n",
    "        try:\n",
    "            # Extract data for the current round\n",
    "            round_data = years_arr[year][round][str(round+1)]\n",
    "            \n",
    "            # Create an empty feature array \n",
    "            round_store = np.zeros([len(teams)*len(variables)], dtype=int)\n",
    "            round_teams = []\n",
    "            \n",
    "            # Iterate over each game in the round data\n",
    "            for game in round_data:\n",
    "                # Extract information about the game\n",
    "                h_team = game['Home']\n",
    "                h_score = int(game['Home_Score'])\n",
    "                a_team = game['Away']\n",
    "                a_score = int(game['Away_Score'])\n",
    "                \n",
    "                # Determine win or lose for each team\n",
    "                h_team_win = h_score >= a_score\n",
    "                a_team_win = a_score >= h_score\n",
    "                \n",
    "                # Determine home team status\n",
    "                h_home = 1\n",
    "                a_home = 0\n",
    "                \n",
    "                # Determine versus index\n",
    "                h_versus = teams.index(a_team)\n",
    "                a_versus = teams.index(h_team)\n",
    "                \n",
    "                # Determine defense (points let in)\n",
    "                h_team_defense = a_score\n",
    "                a_team_defense = h_score  \n",
    "                \n",
    "                # Determine attack points scored\n",
    "                h_team_attack = h_score \n",
    "                a_team_attack = a_score   \n",
    "                \n",
    "                # Determine margin\n",
    "                h_team_margin =  h_score - a_score   \n",
    "                a_team_margin =  a_score - h_score        \n",
    "                \n",
    "                # Keep track of which teams played to work out which teams had a bye \n",
    "                round_teams.append(h_team)\n",
    "                round_teams.append(a_team)\n",
    "                \n",
    "                # Find the index of the team in the overarching array \n",
    "                a_team_idx = teams.index(a_team)\n",
    "                h_team_idx = teams.index(h_team)\n",
    "                \n",
    "                # Determine feature map index\n",
    "                a_team_idx_fm = a_team_idx * len(variables)\n",
    "                h_team_idx_fm = h_team_idx * len(variables)\n",
    "                \n",
    "                # Populate the data for away team\n",
    "                round_store[a_team_idx_fm] = year\n",
    "                round_store[a_team_idx_fm+1] = a_team_win\n",
    "                round_store[a_team_idx_fm+2] = a_team_defense\n",
    "                round_store[a_team_idx_fm+3] = a_team_attack\n",
    "                round_store[a_team_idx_fm+4] = a_team_margin\n",
    "                round_store[a_team_idx_fm+5] = a_home\n",
    "                round_store[a_team_idx_fm+6] = a_versus\n",
    "                round_store[a_team_idx_fm+7] = round+1\n",
    "                \n",
    "                # Populate the data for home team\n",
    "                round_store[h_team_idx_fm] = year\n",
    "                round_store[h_team_idx_fm+1] = h_team_win\n",
    "                round_store[h_team_idx_fm+2] = h_team_defense\n",
    "                round_store[h_team_idx_fm+3] = h_team_attack\n",
    "                round_store[h_team_idx_fm+4] = h_team_margin\n",
    "                round_store[h_team_idx_fm+5] = h_home\n",
    "                round_store[h_team_idx_fm+6] = h_versus\n",
    "                round_store[h_team_idx_fm+7] = round+1\n",
    "                \n",
    "            # Determine teams with a bye\n",
    "            bye_teams = list(set(teams) - set(round_teams))\n",
    "            \n",
    "            # Assign values for teams with a bye\n",
    "            for bye_team in bye_teams:\n",
    "                b_team_idx = teams.index(bye_team)\n",
    "                b_team_idx_fm = b_team_idx * len(variables)\n",
    "                round_store[b_team_idx_fm] = year\n",
    "                round_store[b_team_idx_fm+1] = -1\n",
    "                round_store[b_team_idx_fm+2] = -1\n",
    "                round_store[b_team_idx_fm+3] = -1\n",
    "                round_store[b_team_idx_fm+4] = 0\n",
    "                round_store[b_team_idx_fm+5] = -1\n",
    "                round_store[b_team_idx_fm+6] = -1\n",
    "                round_store[b_team_idx_fm+7] = round+1\n",
    "                \n",
    "            # Append the round data to the all_store list\n",
    "            all_store.append(round_store)\n",
    "            \n",
    "            # Add the new row to the DataFrame using loc\n",
    "            df.loc[len(df)] = round_store\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Feature Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME_HISTORY = 3\n",
    "\n",
    "def get_game_history(year, round_, team):\n",
    "    # Filter the DataFrame by year\n",
    "    filtered_df = df[df[team + \" Year\"] == year]\n",
    "    \n",
    "    # Remove all byes from the game history\n",
    "    filtered_df = filtered_df.iloc[round_-GAME_HISTORY-1:round_-1] \n",
    "    \n",
    "    # Count number of byes\n",
    "    byes = len(filtered_df[filtered_df[team + \" Win\"] == -1])\n",
    "    \n",
    "    # Remove bye rows from the filtered DataFrame\n",
    "    filtered_df = filtered_df[filtered_df[team + \" Win\"] != -1]\n",
    "    \n",
    "    # Calculate mean values for win, defense, attack, and margin\n",
    "    win = filtered_df[team + \" Win\"].mean()\n",
    "    defense = filtered_df[team + \" Defense\"].median()\n",
    "    attack = filtered_df[team + \" Attack\"].median()\n",
    "    margin = filtered_df[team + \" Margin\"].median()\n",
    "    \n",
    "    # Calculate mean values for defense, attack, and margin\n",
    "    defense_mean = filtered_df[team + \" Defense\"].mean()\n",
    "    attack_mean = filtered_df[team + \" Attack\"].mean()\n",
    "    margin_mean = filtered_df[team + \" Margin\"].mean()\n",
    "    \n",
    "    # Calculate the proportion of games played at home\n",
    "    games_at_home = filtered_df[team + \" Home\"].mean()\n",
    "    \n",
    "    return win, defense, attack, margin, byes, games_at_home, defense_mean, attack_mean, margin_mean, year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Learning Data / extending upon the feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "\n",
    "# Input: Team, Other Team Team Stats, Other Team Stats\n",
    "# Output: Team, win/lose, other team, win/lose\n",
    "\n",
    "for team in teams:\n",
    "    # Extract relevant columns from the DataFrame\n",
    "    versed_teams = df[team + \" Versus\"]\n",
    "    wins = df[team + \" Win\"]\n",
    "    rounds = df[team + \" Round\"]\n",
    "    years = df[team + \" Year\"]\n",
    "    margins = df[team + \" Margin\"]\n",
    "    \n",
    "    # Get the index of the current team\n",
    "    c_team_idx = teams.index(team)\n",
    "    \n",
    "    # Iterate over each game in the DataFrame\n",
    "    for versed_team, win, round, year, margin in zip(versed_teams, wins, rounds, years, margins):\n",
    "        # Skip games with byes or games with no momentum\n",
    "        if win == -1 or round <= GAME_HISTORY:\n",
    "            continue\n",
    "        \n",
    "        # Determine the winning team\n",
    "        winning_team = -1\n",
    "        if win == 1:\n",
    "            v_win_ = 0\n",
    "            winning_team = c_team_idx\n",
    "        else:\n",
    "            v_win_ = 1\n",
    "            winning_team = versed_team\n",
    "            \n",
    "        # Check if it's a big win\n",
    "        big_win = 1 if abs(margin) > 13 else 0 \n",
    "        \n",
    "        # Current team, versus team, who wins, current_team_stats, versus_team_stats\n",
    "        X.append([c_team_idx, versed_team, *get_game_history(year, round, team), *get_game_history(year, round, teams[versed_team])])\n",
    "        y.append([c_team_idx, versed_team, win, v_win_, big_win])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest Regressor model\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y), test_size=0.3, shuffle=True)\n",
    "\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X_train, X_val, y_train, y_val = np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "\n",
    "# Create a neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))  # Dropout layer to reduce overfitting\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))  # Dropout layer to reduce overfitting\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(5))\n",
    "\n",
    "# Training loop to track R-squared scores on both training and validation sets\n",
    "num_epochs = 1000  # You can adjust the number of epochs as needed\n",
    "# Define a learning rate schedule\n",
    "initial_learning_rate = 0.00001\n",
    "final_learning_rate = 0.001\n",
    "batch_size = 32\n",
    "learning_rate_decay_factor = (final_learning_rate / initial_learning_rate)**(1/num_epochs)\n",
    "steps_per_epoch = int(len(X_train_scaled)/batch_size)\n",
    "\n",
    "lr_schedule = ExponentialDecay(\n",
    "                initial_learning_rate=initial_learning_rate,\n",
    "                decay_steps=steps_per_epoch,\n",
    "                decay_rate=learning_rate_decay_factor,\n",
    "                staircase=True)\n",
    "previous_loss = None\n",
    "no_loss_change_epochs = 0\n",
    "loss_change_threshold = 1e-5\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=lr_schedule), loss='mse')\n",
    "\n",
    "# Lists to store the R-squared scores during training\n",
    "train_r2_scores = []\n",
    "val_r2_scores = []\n",
    "train_losses = []  \n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train the model on the training data\n",
    "    history = model.fit(X_train_scaled, y_train, batch_size=batch_size, epochs=1, verbose=2)\n",
    "    \n",
    "    # Calculate R-squared scores on the training and validation sets\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    \n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Store the R-squared scores for each epoch\n",
    "    train_r2_scores.append(train_r2)\n",
    "    val_r2_scores.append(val_r2)\n",
    "    \n",
    "    \n",
    "    train_loss = history.history['loss'][0]\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    if previous_loss is not None and abs(previous_loss - train_loss) < loss_change_threshold:\n",
    "        no_loss_change_epochs += 1\n",
    "    else:\n",
    "        no_loss_change_epochs = 0\n",
    "    \n",
    "    # Set the current loss as the previous loss for the next epoch\n",
    "    previous_loss = train_loss\n",
    "    \n",
    "    # If there have been no loss changes for a certain number of consecutive epochs, stop training\n",
    "    if no_loss_change_epochs >= 5:\n",
    "        print(f\"Training stopped early at epoch {epoch + 1} due to no significant loss change.\")\n",
    "        break\n",
    "    \n",
    "\n",
    "# Final R-squared scores\n",
    "final_train_r2, final_val_r2 = train_r2_scores[-1], val_r2_scores[-1]\n",
    "\n",
    "print(f\"Final Training R-squared: {final_train_r2:.4f}\")\n",
    "print(f\"Final Validation R-squared: {final_val_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising the ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting the Loss\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting R-squared scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_r2_scores, label='Training R-squared')\n",
    "plt.plot(val_r2_scores, label='Validation R-squared')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('R-squared Score')\n",
    "plt.title('R-squared Score over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of true vs. predicted values\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "y_val_pred = model.predict(X_val_scaled)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "axs[0, 0].scatter(y_train[:, 0], y_train_pred[:, 0], alpha=0.5)\n",
    "axs[0, 0].set_title('Target 1')\n",
    "axs[0, 1].scatter(y_train[:, 1], y_train_pred[:, 1], alpha=0.5)\n",
    "axs[0, 1].set_title('Target 2')\n",
    "axs[1, 0].scatter(y_train[:, 2], y_train_pred[:, 2], alpha=0.5)\n",
    "axs[1, 0].set_title('Target 3')\n",
    "axs[1, 1].scatter(y_train[:, 3], y_train_pred[:, 3], alpha=0.5)\n",
    "axs[1, 1].set_title('Target 4')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='True Values', ylabel='Predicted Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "wkd_matches = [[\"Broncos\", \"Rabbitohs\"], [\"Sharks\", \"Bulldogs\"], [\"Panthers\", \"Eels\"], [\"Raiders\", \"West Tigers\"], [\"Cowboys\", \"Knights\"], [\"Storm\", \"Warriors\"], [\"Sea Eagles\", \"Roosters\"], [\"Dolphins\", \"Dragons\"]]\n",
    "\n",
    "# Iterate over each weekend match\n",
    "for wkd_match in wkd_matches:\n",
    "    # Extract the teams\n",
    "    team_1 = int(teams.index(wkd_match[0]))\n",
    "    team_2 = int(teams.index(wkd_match[1]))\n",
    "\n",
    "    # Prepare input data for prediction\n",
    "    pred_in_2 = [team_1, team_2,  *get_game_history(2023, 22, teams[int(team_1)]), *get_game_history(2023, 22, teams[int(team_2)])]\n",
    "\n",
    "    # Get predictions from the model\n",
    "    predictions = model.predict([pred_in_2], verbose=0)\n",
    "    predictions = predictions[0]\n",
    "    \n",
    "    # Determine the winner based on predictions\n",
    "    if predictions[2] > predictions[3]:\n",
    "        print(f\"{teams[team_1]} wins\\t\\t {teams[team_1]}: {predictions[2]:.4f}\\t{teams[team_2]}: {predictions[3]:.4f}\\t\\tBig Win {predictions[4]}\")\n",
    "    else:\n",
    "        print(f\"{teams[team_2]} wins\\t\\t {teams[team_1]}: {predictions[2]:.4f}\\t{teams[team_2]}: {predictions[3]:.4f}\\t\\tBig Win {predictions[4]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
